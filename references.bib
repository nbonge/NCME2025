@article{Bassili1996,
   abstract = {This research explores the potential utility of response latency as an index of question problems in survey research. The time respondents took to answer three types of bad questions was compared to the time they took to answer the repaired versions of the questions. Questions containing a superfluous negative and double-barreled questions took longer to answer than nearly identical questions without these problems. Repaired versions of questions soliciting frequency estimates, however, took longer to answer than their problematic counterparts. The results are discussed in the context of a model of question answering, and their implications for survey methodology are explored. Survey methodologists have expressed considerable interest recently in techniques for screening survey questions with the aim of repairing bad questions before presenting them to large numbers of respondents (see Presser and Blair 1994). A number of approaches for the early identification of question problems have been explored. Observational monitoring focuses on the interaction between the interviewer and respondent and relies on a behavior-coding scheme to identify problems (e.g., Fowler and Cannell 19%). The cognitive interview is a method for gathering detailed information from respondents about the processes involved in the formulation of responses and involves extensive probing, either during the interview or immediately after it (e.g., Jobe, Tourangeau, and Smith 1993). Analysis of the verbal output based on "think aloud" protocols obtained from respondents retrospectively or while they answer questions has also been implemented, sometimes with automatic coding of the protocols (e.g., Bolton 1993). In addition, methods for coding the questionnaire itself have also been developed (e.g., Lessler and Forsyth 1996).},
   author = {John N Bassili and B Stacey Scott},
   isbn = {60/3/390/1832313},
   journal = {Public Opinion Quarterly},
   pages = {390-399},
   title = {Response latency as a signal to question problems in survey research},
   volume = {60},
   url = {https://academic.oup.com/poq/article/60/3/390/1832313},
   year = {1996}
}
@article{Bates2015,
   author = {Douglas Bates and Martin Mächler and Ben Bolker and Steve Walker},
   doi = {10.18637/jss.v067.i01},
   issn = {1548-7660},
   issue = {1},
   journal = {Journal of Statistical Software},
   pages = {1-48},
   title = {Fitting linear mixed-effects models using lme4},
   volume = {67},
   year = {2015}
}
@article{Beckmann2000,
   author = {Jens F. Beckmann},
   issue = {3},
   journal = {Diagnostica},
   pages = {124-129},
   title = {Differentielle latenzzeitefekte bei der bearbeitung von reasoning-items},
   volume = {46},
   year = {2000}
}
@article{Bowling2023,
   abstract = {Several recent studies have examined the prevention, causes, and consequences of insufficient effort responding (IER) to surveys. Scientific progress in this area, however, rests on the availability of construct-valid IER measures. In the current paper we describe the potential merits of the page time index, which is computed by counting the number of questionnaire pages to which a participant has responded more quickly than two seconds per item (see Huang et al., 2012). We conducted three studies (total N = 1,056) to examine the page time index's construct validity. Across these studies, we found that page time converged highly with other IER indices, that it was sensitive to an experimental manipulation warning participants to respond carefully, and that it predicted the extent to which participants were unable to recognize item content. We also found that page time's validity was superior to that of total completion time and that the two-seconds-per-item rule yielded a construct-valid page time score for items of various word lengths. Given its apparent validity, we provide practical recommendations for the use of the page time index.},
   author = {Nathan A. Bowling and Jason L. Huang and Cheyna K. Brower and Caleb B. Bragg},
   doi = {10.1177/10944281211056520},
   issn = {15527425},
   issue = {2},
   journal = {Organizational Research Methods},
   keywords = {careless responding,insufficient effort responding,participant inattention,random responding,response effort},
   month = {4},
   pages = {323-352},
   publisher = {SAGE Publications Inc.},
   title = {The quick and the careless: The construct validity of page time as a measure of insufficient effort responding to surveys},
   volume = {26},
   year = {2023}
}
@article{Chalmers2012,
   author = {R. Philip Chalmers},
   doi = {10.18637/jss.v048.i06},
   issn = {1548-7660},
   issue = {6},
   journal = {Journal of Statistical Software},
   pages = {1-29},
   title = {mirt: A multidimensional item response theory package for the R environment},
   volume = {48},
   year = {2012}
}
@article{Chan2022,
   abstract = {We examined the influences of pre-solving pause time, algebraic knowledge, mathematics self-efficacy, and mathematics anxiety on middle-schoolers' strategy efficiency in an algebra learning game. We measured strategy efficiency using (a) the number of steps taken to complete a problem, (b) the proportion of problems completed on the initial attempt, and (c) the number of resets prior to completing the problems. Using the log data from the game, we found that longer pre-solving pause time was associated with more efficient strategies, as indicated by fewer solution steps, higher initial completion rate, and fewer resets. Higher algebraic knowledge was associated with higher initial completion rate and fewer resets. Mathematics self-efficacy and mathematics anxiety was not associated with any measures of strategy efficiency. The results suggest that pause time may be an indicator of student thinking before problem-solving, and provide insights into using data from online learning platforms to examine students' problem-solving processes.},
   author = {Jenny Yun Chen Chan and Erin R. Ottmar and Ji Eun Lee},
   doi = {10.1016/j.lindif.2021.102109},
   issn = {18733425},
   journal = {Learning and Individual Differences},
   keywords = {Algebra problem-solving,Metacognitive skills,Online learning environment,Pause time,Strategy efficiency},
   month = {1},
   publisher = {Elsevier Ltd},
   title = {Slow down to speed up: Longer pause time before solving problems relates to higher strategy efficiency},
   volume = {93},
   year = {2022}
}
@article{Cred2017,
   abstract = {Grit has been presented as a higher order personality trait that is highly predictive of both success and performance and distinct from other traits such as conscientiousness. This paper provides a meta-analytic review of the grit literature with a particular focus on the structure of grit and the relation between grit and performance, retention, conscientiousness, cognitive ability, and demographic variables. Our results based on 584 effect sizes from 88 independent samples representing 66,807 individuals indicate that the higher order structure of grit is not confirmed, that grit is only moderately correlated with performance and retention, and that grit is very strongly correlated with conscientiousness. We also find that the perseverance of effort facet has significantly stronger criterion validities than the consistency of interest facet and that perseverance of effort explains variance in academic performance even after controlling for conscientiousness. In aggregate our results suggest that interventions designed to enhance grit may only have weak effects on performance and success, that the construct validity of grit is in question, and that the primary utility of the grit construct may lie in the perseverance facet.},
   author = {Marcus Credé and Michael C. Tynan and Peter D. Harms},
   doi = {10.1037/pspp0000102},
   issn = {00223514},
   issue = {3},
   journal = {Journal of Personality and Social Psychology},
   keywords = {Consistency of interest,Grit,Meta-analysis,Performance,Perseverance of effort},
   month = {9},
   pages = {492-511},
   pmid = {27845531},
   publisher = {American Psychological Association Inc.},
   title = {Much ado about grit: A meta-analytic synthesis of the grit literature},
   volume = {113},
   year = {2017}
}
@article{DeBoeck2019,
   abstract = {Response times (RTs) are a natural kind of data to investigate cognitive processes underlying cognitive test performance. We give an overview of modeling approaches and of findings obtained with these approaches. Four types of models are discussed: response time models (RT as the sole dependent variable), joint models (RT together with other variables as dependent variable), local dependency models (with remaining dependencies between RT and accuracy), and response time as covariate models (RT as independent variable). The evidence from these approaches is often not very informative about the specific kind of processes (other than problem solving, information accumulation, and rapid guessing), but the findings do suggest dual processing: automated processing (e.g., knowledge retrieval) vs. controlled processing (e.g., sequential reasoning steps), and alternative explanations for the same results exist. While it seems well-possible to differentiate rapid guessing from normal problem solving (which can be based on automated or controlled processing), further decompositions of response times are rarely made, although possible based on some of model approaches.},
   author = {Paul De Boeck and Minjeong Jeon},
   doi = {10.3389/fpsyg.2019.00102},
   issn = {16641078},
   issue = {FEB},
   journal = {Frontiers in Psychology},
   keywords = {Automated and controlled processes,Cognitive processes,Cognitive tests,Local dependencies,Psychometric models,Response accuracy,Response time},
   month = {2},
   publisher = {Frontiers Media S.A.},
   title = {An overview of models for response times and processes in cognitive tests},
   volume = {10},
   year = {2019}
}
@article{Duckworth2007,
   abstract = {The importance of intellectual talent to achievement in all professional domains is well established, but less is known about other individual differences that predict success. The authors tested the importance of 1 noncognitive trait: grit. Defined as perseverance and passion for long-term goals, grit accounted for an average of 4% of the variance in success outcomes, including educational attainment among 2 samples of adults (N = 1,545 and N = 690), grade point average among Ivy League undergraduates (N = 138), retention in 2 classes of United States Military Academy, West Point, cadets (N = 1,218 and N = 1,308), and ranking in the National Spelling Bee (N = 175). Grit did not relate positively to IQ but was highly correlated with Big Five Conscientiousness. Grit nonetheless demonstrated incremental predictive validity of success measures over and beyond IQ and conscientiousness. Collectively, these findings suggest that the achievement of difficult goals entails not only talent but also the sustained and focused application of talent over time. © 2007 American Psychological Association.},
   author = {Angela L. Duckworth and Christopher Peterson and Michael D. Matthews and Dennis R. Kelly},
   doi = {10.1037/0022-3514.92.6.1087},
   issn = {00223514},
   issue = {6},
   journal = {Journal of Personality and Social Psychology},
   keywords = {achievement,performance,persistence,personality,success},
   month = {6},
   pages = {1087-1101},
   pmid = {17547490},
   title = {Grit: Perseverance and passion for long-term goals},
   volume = {92},
   year = {2007}
}
@article{Ferrando2007,
   abstract = {This article describes a model for response times that is proposed as a supplement to the usual factor-analytic model for responses to graded or more continuous typical-response items. The use of the proposed model together with the factor model provides additional information about the respondent and can potentially increase the accuracy of the individual trait estimates. First, the rationale of the model is discussed in relation to previous developments in binary responses. Sec- ond, procedures for fitting the model and for assessing model-data fit at both overall and individual level (person-fit) are proposed. Third, the usefulness of the model and its potential applications in the typical-response domain are discussed. All the proposed developments are used in 2 empirical applications in the person- ality domain. The first application analyzes 2 scales from a Big Five questionnaire. The second example analyzes a sociability scale developed from Eysenck’s ques- tionnaires.},
   author = {Pere J. Ferrando and Urbano Lorenzo-Seva},
   doi = {10.1080/00273170701710247},
   issn = {0027-3171},
   issue = {4},
   journal = {Multivariate Behavioral Research},
   month = {12},
   pages = {675-706},
   publisher = {Informa UK Limited},
   title = {A measurement model for Likert responses that incorporates response time},
   volume = {42},
   year = {2007}
}
@book{Fox2019,
   author = {John Fox and Sanford Weisberg},
   city = {Thousand Oaks, CA},
   edition = {Third},
   publisher = {Sage},
   title = {An R companion to applied regression},
   year = {2019}
}
@article{Gernsbacher2020,
   abstract = {For more than a century, measurement experts have distinguished between time-limited tests and untimed power tests, which are administered without time limits or with time limits so generous that all students are assured of completing all items. On untimed power tests, students can differ in their propensity to correctly respond to every item, and items should differ in how many correct responses they elicit. However, differences among students' speed of responding do not confound untimed power tests; therefore, untimed power tests ensure more accurate assessment. In this article, we present four empirically based reasons to administer untimed power tests rather than time-limited tests in educational settings. (1) Time-limited tests are less valid; students' test-taking pace is not a valid reflection of their knowledge and mastery. (2) Time-limited tests are less reliable; estimates of time-limited tests' reliability are artificially inflated due to artifactual consistency in students' rate of work rather than authentic consistency in students' level of knowledge. (3) Time-limited tests are less inclusive; time-limited tests exclude students with documented disabilities who, because they are legally allowed additional test-taking time, are often literally excluded from test-taking classrooms. (4) Time-limited tests are less equitable; in addition to excluding students with documented disabilities, time-limited tests can also impede students who are learning English, students from underrepresented backgrounds, students who are older than average, and students with disabilities who encounter barriers (e.g., stigma and financial expense) in obtaining disability documentation and legally mandated accommodations. We conclude by offering recommendations for avoiding time-limited testing in higher educational assessment.},
   author = {Morton Ann Gernsbacher and Raechel N. Soicher and Kathryn A. Becker-Blease},
   doi = {10.1037/tps0000232},
   issn = {2332-2136},
   issue = {2},
   journal = {Translational Issues in Psychological Science},
   month = {6},
   pages = {175-190},
   publisher = {American Psychological Association (APA)},
   title = {Four empirically based reasons not to administer time-limited tests.},
   volume = {6},
   year = {2020}
}
@article{Hhne2017,
   abstract = {Measuring attitudes and opinions employing agree/disagree (A/D) questions is a common method in social research because it appears to be possible to measure different constructs with identical response scales. However, theoretical considerations suggest that A/D questions require a considerable cognitive processing. Item-specific (IS) questions, in contrast, offer content-related response categories, implying less cognitive processing. To investigate the respective cognitive effort and response quality associated with A/D and IS questions, we conducted a web-based experiment with 1,005 students. Cognitive effort was assessed by response times and answer changes. Response quality, in contrast, was assessed by different indicators such as dropouts. According to our results, single IS questions require higher cognitive effort than single A/D questions in terms of response times. Moreover, our findings show substantial differences in processing single and grid questions.},
   author = {Jan Karem Höhne and Stephan Schlosser and Dagmar Krebs},
   doi = {10.1177/1525822X17710640},
   issn = {15523969},
   issue = {4},
   journal = {Field Methods},
   month = {11},
   pages = {365-382},
   publisher = {SAGE Publications Inc.},
   title = {Investigating cognitive effort and response quality of question formats in web surveys using paradata},
   volume = {29},
   year = {2017}
}
@article{Krosnick1991,
   abstract = {This paper proposes that when optimally answering a survey question would require substantial cognitive effort, some repondents simply provide a satisfactory answer instead. This behaviour, called satisficing, can take the form of either (1) incomplete or biased information retrieval and/or information integration, or (2) no information retrieval or integration at all. Satisficing may lead respondents to employ a variety of response strategies, including choosing the first response alternative that seems to constitute a reasonable answer, agreeing with an assertion made by a question, endorsing the status quo instead of endorsing social change, failing to differentiate among a set of diverse objects in ratings, saying ‘don't know’ instead of reporting an opinion, and randomly choosing among the response alternatives offered. This paper specifies a wide range of factors that are likely to encourage satisficing, and reviews relevant evidence evaluating these speculations. Many useful directions for future research are suggested. Copyright © 1991 John Wiley & Sons, Ltd},
   author = {Jon A. Krosnick},
   doi = {10.1002/acp.2350050305},
   issn = {10990720},
   issue = {3},
   journal = {Applied Cognitive Psychology},
   pages = {213-236},
   title = {Response strategies for coping with the cognitive demands of attitude measures in surveys},
   volume = {5},
   year = {1991}
}
@article{Kuznetsova2017,
   author = {Alexandra Kuznetsova and Per B. Brockhoff and Rune H. B. Christensen},
   doi = {10.18637/jss.v082.i13},
   issn = {1548-7660},
   issue = {13},
   journal = {Journal of Statistical Software},
   pages = {1-26},
   title = {lmerTest Package: Tests in linear mixed effects models},
   volume = {82},
   year = {2017}
}
@article{Lenzner2012,
   abstract = {Many studies have shown that vague or ambiguous questions are often interpreted idiosyncratically by respondents and thus can increase measurement error. This article provides some evidence that the cognitive effort required to comprehend survey questions affects data quality in a similar way. A web survey experiment revealed that respondents receiving less comprehensible questions provided lower-quality responses (as indicated by breakoff rates, number of nonsubstantive responses, number of neutral responses, and over-time consistency) than respondents receiving control questions that were easier to comprehend. Moreover, interaction effects of question comprehensibility with respondents' verbal skills and their motivation to answer surveys were found. These findings indicate that survey designers should minimize the cognitive effort required to comprehend their questions and the article suggests specific ways how to do so. © The Author(s) 2012.},
   author = {Timo Lenzner},
   doi = {10.1177/1525822X12448166},
   issn = {1525822X},
   issue = {4},
   journal = {Field Methods},
   keywords = {question wording,questionnaire design,response effects,response quality,web survey},
   month = {11},
   pages = {409-428},
   title = {Effects of survey question comprehensibility on response quality},
   volume = {24},
   year = {2012}
}
@article{Lenzner2010,
   abstract = {An important objective in survey question design is to write clear questions that respondents find easy to understand and to answer. This contribution identifies the factors that influence question clarity. Theoretical and empirical evidence from psycholinguistics suggests that specific text features (e.g., low-frequency words (LFRWs), left-embedded syntax) cause comprehension difficulties and impose a high cognitive burden on respondents. To examine the effect of seven different text features on question clarity, an online experiment was conducted in which well-formulated questions were compared to suboptimal counterparts. The cognitive burden of the questions was assessed with response times. Data quality was compared in terms of drop-out rates and survey satisficing behaviour. The results show that at least six of the text features are relevant for the clarity of a question. We provide a detailed explanation of these text features and advise survey designers to avoid them when crafting questions. Copyright © 2009 John Wiley & Sons, Ltd. Copyright © 2009 John Wiley & Sons, Ltd.},
   author = {Timo Lenzner and Lars Kaczmirek and Alwine Lenzner},
   doi = {10.1002/acp.1602},
   issn = {08884080},
   issue = {7},
   journal = {Applied Cognitive Psychology},
   month = {10},
   pages = {1003-1020},
   title = {Cognitive burden of survey questions and response times: A psycholinguistic experiment},
   volume = {24},
   year = {2010}
}
@misc{Mullis2020,
   author = {I. V. S. Mullis and M. O. Martin and P. Foy and D. L. Kelly and B. Fishbein},
   journal = {Boston College, TIMSS & PIRLS International Study Center website},
   title = {TIMSS 2019 International Results in Mathematics and Science},
   url = {https://timssandpirls.bc.edu/timss2019/international-results/},
   year = {2020}
}
@article{Nagy2022,
   abstract = {Disengaged item responses pose a threat to the validity of the results provided by large-scale assessments. Several procedures for identifying disengaged responses on the basis of observed response times have been suggested, and item response theory (IRT) models for response engagement have been proposed. We outline that response time-based procedures for classifying response engagement and IRT models for response engagement are based on common ideas, and we propose the distinction between independent and dependent latent class IRT models. In all IRT models considered, response engagement is represented by an item-level latent class variable, but the models assume that response times either reflect or predict engagement. We summarize existing IRT models that belong to each group and extend them to increase their flexibility. Furthermore, we propose a flexible multilevel mixture IRT framework in which all IRT models can be estimated by means of marginal maximum likelihood. The framework is based on the widespread Mplus software, thereby making the procedure accessible to a broad audience. The procedures are illustrated on the basis of publicly available large-scale data. Our results show that the different IRT models for response engagement provided slightly different adjustments of item parameters of individuals’ proficiency estimates relative to a conventional IRT model.},
   author = {Gabriel Nagy and Esther Ulitzsch},
   doi = {10.1177/00131644211045351},
   issn = {15523888},
   issue = {5},
   journal = {Educational and Psychological Measurement},
   keywords = {item response theory,multilevel mixture modeling,response engagement,response time},
   month = {10},
   pages = {845-879},
   publisher = {SAGE Publications Inc.},
   title = {A multilevel mixture IRT framework for modeling response times as predictors or indicators of response engagement in IRT models},
   volume = {82},
   year = {2022}
}
@phdthesis{Nguyen2017,
   author = {Hung Loan T. Nguyen},
   school = {Middle Tennessee State University},
   title = {Tired of survey fatigue? Insufficient effort responding due to survey fatigue},
   year = {2017}
}
@misc{RCoreTeam2019,
   author = {R Core Team},
   city = {Vienna, Austria},
   publisher = {R Foundation for Statistical Computing},
   title = {R: A language and environment for statistical computing},
   url = {http://www.R-project.org/},
   year = {2019}
}
@article{Steinmayr2018,
   abstract = {Grit—individuals’ perseverance of effort and consistency of interests—was introduced in 2007 as new construct that predicts different achievement outcomes. To date, most studies examining grit's prediction of achievement have not included other predictors in their analyses. Therefore, we assessed grit's incremental validity for school achievement above theoretically and empirically related predictors, in two adolescent student samples from Germany. Study 1 (N = 227) examined grit's relative importance for students’ school grades (GPA, math, German) when controlling for prior school grades, the Big Five personality traits, school engagement, values, expectancies for success, and self-efficacy. In Study 2 (N = 586), intelligence, conscientiousness, and established constructs from motivation and engagement literatures were controlled to investigate grit's relative importance for GPA, math grades and test performance in math. In both studies, relative weight analyses revealed that the grit subscales added little explanatory power. Results question grit's unique prediction of scholastic success.},
   author = {Ricarda Steinmayr and Anne F. Weidinger and Allan Wigfield},
   doi = {10.1016/j.cedpsych.2018.02.004},
   issn = {10902384},
   journal = {Contemporary Educational Psychology},
   keywords = {Grit,Intelligence,Motivation,Personality,Relative weight analysis,School engagement,School performance},
   month = {4},
   pages = {106-122},
   publisher = {Academic Press Inc.},
   title = {Does students’ grit predict their school achievement above and beyond their personality, motivation, and engagement?},
   volume = {53},
   year = {2018}
}
@article{Tanco2023,
   abstract = {The stereotype that children who are more able solve tasks quicker than their less capable peers exists both in and outside education. The F > C phenomenon and the distance–difficulty hypothesis offer alternative explanations of the time needed to complete a task; the former by the response correctness and the latter by the relative difference between the difficulty of the task and the ability of the examinee. To test these alternative explanations, we extracted IRT-based ability estimates and task difficulties from a sample of 514 children, 53% girls, M(age) = 10.3 years; who answered 29 Piagetian balance beam tasks. We used the answer correctness and task difficulty as predictors in multilevel regression models when controlling for children’s ability levels. Our results challenge the ‘faster equals smarter’ stereotype. We show that ability levels predict the time needed to solve a task when the task is solved incorrectly, though only with moderately and highly difficult items. Moreover, children with higher ability levels take longer to answer items incorrectly, and tasks equal to children’s ability levels take more time than very easy or difficult tasks. We conclude that the relationship between ability, task difficulty, and answer correctness is complex, and warn education professionals against basing their professional judgment on students’ quickness.},
   author = {Martin Tancoš and Edita Chvojka and Michal Jabůrek and Šárka Portešová},
   doi = {10.3390/jintelligence11040063},
   issn = {20793200},
   issue = {4},
   journal = {Journal of Intelligence},
   keywords = {F > C phenomenon,IRT,Thissen’s model,balance beam task,distance–difficulty hypothesis,fluid intelligence,game-based assessment,response time},
   month = {4},
   publisher = {MDPI},
   title = {Faster ≠ smarter: Children with higher levels of ability take longer to give incorrect answers, especially when the task matches their ability},
   volume = {11},
   year = {2023}
}
@inbook{Thissen1983,
   author = {David Thissen},
   booktitle = {New horizons in testing: Latent trait test theory and computerized adaptive testing},
   pages = {179-203},
   title = {Timed testing- An approach using item response theory},
   year = {1983}
}
@article{Ulitzsch2022,
   abstract = {Careless and insufficient effort responding (C/IER) can pose a major threat to data quality and, as such, to validity of inferences drawn from questionnaire data. A rich body of methods aiming at its detection has been developed. Most of these methods can detect only specific types of C/IER patterns. However, typically different types of C/IER patterns occur within one data set and need to be accounted for. We present a model-based approach for detecting manifold manifestations of C/IER at once. This is achieved by leveraging response time (RT) information available from computer-administered questionnaires and integrating theoretical considerations on C/IER with recent psychometric modeling approaches. The approach a) takes the specifics of attentive response behavior on questionnaires into account by incorporating the distance–difficulty hypothesis, b) allows for attentiveness to vary on the screen-by-respondent level, c) allows for respondents with different trait and speed levels to differ in their attentiveness, and d) at once deals with various response patterns arising from C/IER. The approach makes use of item-level RTs. An adapted version for aggregated RTs is presented that supports screening for C/IER behavior on the respondent level. Parameter recovery is investigated in a simulation study. The approach is illustrated in an empirical example, comparing different RT measures and contrasting the proposed model-based procedure against indicator-based multiple-hurdle approaches.},
   author = {Esther Ulitzsch and Steffi Pohl and Lale Khorramdel and Ulf Kroehne and Matthias von Davier},
   doi = {10.1007/s11336-021-09817-7},
   issn = {18600980},
   issue = {2},
   journal = {Psychometrika},
   keywords = {careless responses,data screening,item response theory,mixture modeling,response times},
   month = {6},
   pages = {593-619},
   pmid = {34855118},
   publisher = {Springer},
   title = {A response-time-based latent response mixture model for identifying and modeling careless and insufficient effort responding in survey data},
   volume = {87},
   year = {2022}
}
@techReport{Wise2006,
   author = {Steven L Wise and Christine E Demars},
   issue = {1},
   journal = {Source: Journal of Educational Measurement},
   pages = {19-38},
   title = {An Application of Item Response Time: The Effort-Moderated IRT Model National Council on Measurement in Education},
   volume = {43},
   url = {https://www.jstor.org/stable/20461807},
   year = {2006}
}
